**Paper title:**

ZCOMP: Reducing DNN Cross-Layer Memory Footprint Using Vector Extensions

**Publication:**

MICROâ€™19

**Problem to solve:**

1. Prior works have observed that DNNs exhibit significant data sparsity, making
them amenable to data compression techniques.

2. Prior works rely on fully-customized datapaths in order to encode or skip
zero values in the activation data communicated between network layers. They are
challenging for general purpose multi-processor CPUs that need to preserve
legacy support for virtual memory and hardware-managed coherent cache
hierarchies.

3. When tighter integration is desired between DNN and non-DNN tasks, CPUs are
preferable due to their flexibility, lower latency, and availability in the
datacenter. It is important to develop CPU-tailored optimizations for addressing
the memory challenges imposed by DNN workloads.

**Major contribution:**

This paper explore instruction set architecture (ISA) extensions called ZCOMP
for efficiently compressing dynamic data generated by DNNs.

The techniques are motivated by two key observations: First, feature maps
produced by DNN layers often include large populations of zero value activations
interspersed with non-zero activations. Second, feature map accesses almost
always exhibit perfect streaming patterns, whereby activation data is
sequentially written by one layer and later read sequentially by another layer,
without requiring random element retrievals.

The function of ZCOMP is to dynamically compress/expand data in a memory region
before being written/read to/from the memory hierarchy. ZCOMP enables zero data
values in feature maps to be compactly encoded, while fully automating the
meta-data generation, storage and retrieval, which eliminates the need for extra
instruction executions and register.

Compared to an uncompressed baseline, ZCOMP reduces average memory traffic by
31% and 23% for training and inference, respectively, leading to an average 11%
and 3% performance improvements. This performance improvement is much higher
than the 4% (training) and -2% (inference) achieved by AVX512 vector extensions.

2. The header of ZCOMP instructions interact differently with the virtual
memory. If the predicted compress radio in the virtual memory is low, they make
the data in virtual unchanged.
